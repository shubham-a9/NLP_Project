{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural_translation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-a9/NLP_Project/blob/main/Neural_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVgRIXKwmAtS"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5E3rvx6maoo",
        "outputId": "62703538-3058-4d20-b1c8-6a35a0e2bbbc"
      },
      "source": [
        "!pip install Faker\n",
        "\n",
        "from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(12345)\n",
        "random.seed(12345)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Faker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/dc/266528ca3f7e6c14016832b955adc4313b9c18529a5012e13c42b858c33f/Faker-8.1.1-py3-none-any.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode==1.3 in /usr/local/lib/python3.7/dist-packages (from Faker) (1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.7/dist-packages (from Faker) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.4->Faker) (1.15.0)\n",
            "Installing collected packages: Faker\n",
            "Successfully installed Faker-8.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rgKWgxUvnn9"
      },
      "source": [
        "# Define format of the data we would like to generate\n",
        "FORMATS = ['short',\n",
        "           'medium',\n",
        "           'long',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'full',\n",
        "           'd MMM YYY', \n",
        "           'd MMMM YYY',\n",
        "           'dd MMM YYY',\n",
        "           'd MMM, YYY',\n",
        "           'd MMMM, YYY',\n",
        "           'dd, MMM YYY',\n",
        "           'd MM YY',\n",
        "           'd MMMM YYY',\n",
        "           'MMMM d YYY',\n",
        "           'MMMM d, YYY',\n",
        "           'dd.MM.YY']\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7Em6s5JtY__"
      },
      "source": [
        "def load_dataset(m):\n",
        "    \"\"\"\n",
        "        Loads a dataset with m examples and vocabularies\n",
        "        :m: the number of examples to generate\n",
        "    \"\"\"\n",
        "    \n",
        "    human_vocab = set()\n",
        "    machine_vocab = set()\n",
        "    dataset = []\n",
        "    Tx = 30\n",
        "    \n",
        "\n",
        "    for i in tqdm(range(m)):\n",
        "        h, m, _ = load_date()\n",
        "        if h is not None:\n",
        "            dataset.append((h, m))\n",
        "            human_vocab.update(tuple(h))\n",
        "            machine_vocab.update(tuple(m))\n",
        "    \n",
        "    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], \n",
        "                     list(range(len(human_vocab) + 2))))\n",
        "    inv_machine = dict(enumerate(sorted(machine_vocab)))\n",
        "    machine = {v:k for k,v in inv_machine.items()}\n",
        " \n",
        "    return dataset, human, machine, inv_machine\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stQu_k3xtjaY"
      },
      "source": [
        "def load_date():\n",
        "    \"\"\"\n",
        "        Loads some fake dates \n",
        "        :returns: tuple containing human readable string, machine readable string, and date object\n",
        "    \"\"\"\n",
        "    dt = fake.date_object()\n",
        "\n",
        "    try:\n",
        "        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US')\n",
        "        human_readable = human_readable.lower()\n",
        "        human_readable = human_readable.replace(',','')\n",
        "        machine_readable = dt.isoformat()\n",
        "        \n",
        "    except AttributeError as e:\n",
        "        return None, None, None\n",
        "\n",
        "    return human_readable, machine_readable, dt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-P6pQZrwT8i"
      },
      "source": [
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
        "    \n",
        "    X, Y = zip(*dataset)\n",
        "    \n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n",
        "    \n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n",
        "\n",
        "    return X, np.array(Y), Xoh, Yoh"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i80V7R92gdg"
      },
      "source": [
        "def string_to_int(string, length, vocab):\n",
        "    \"\"\"\n",
        "    Converts all strings in the vocabulary into a list of integers representing the positions of the\n",
        "    input string's characters in the \"vocab\"\n",
        "    \n",
        "    Arguments:\n",
        "    string -- input string, e.g. 'Wed 10 Jul 2007'\n",
        "    length -- the number of time steps you'd like, determines if the output will be padded or cut\n",
        "    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n",
        "    \n",
        "    Returns:\n",
        "    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n",
        "    \"\"\"\n",
        "    \n",
        "    #make lower to standardize\n",
        "    string = string.lower()\n",
        "    string = string.replace(',','')\n",
        "    \n",
        "    if len(string) > length:\n",
        "        string = string[:length]\n",
        "        \n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n",
        "    \n",
        "    if len(string) < length:\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\n",
        "    \n",
        "    #print (rep)\n",
        "    return rep\n",
        "\n",
        "\n",
        "def int_to_string(ints, inv_vocab):\n",
        "    \"\"\"\n",
        "    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n",
        "    \n",
        "    Arguments:\n",
        "    ints -- list of integers representing indexes in the machine's vocabulary\n",
        "    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters \n",
        "    \n",
        "    Returns:\n",
        "    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n",
        "    \"\"\"\n",
        "    \n",
        "    l = [inv_vocab[i] for i in ints]\n",
        "    return l"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzqZV4qE2nLf"
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "\n",
        "def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n",
        "    encoded = string_to_int(text, Tx, input_vocabulary)\n",
        "    prediction = model.predict(np.array([encoded]))\n",
        "    prediction = np.argmax(prediction[0], axis=-1)\n",
        "    return int_to_string(prediction, inv_output_vocabulary)\n",
        "\n",
        "def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n",
        "    predicted = []\n",
        "    for example in examples:\n",
        "        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n",
        "        print('input:', example)\n",
        "        print('output:', predicted[-1])\n",
        "    return predicted"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc12wZ_f2wff"
      },
      "source": [
        "def softmax(x, axis=1):\n",
        "    \"\"\"Softmax activation function.\n",
        "    # Arguments\n",
        "        x : Tensor.\n",
        "        axis: Integer, axis along which the softmax normalization is applied.\n",
        "    # Returns\n",
        "        Tensor, output of softmax transformation.\n",
        "    # Raises\n",
        "        ValueError: In case `dim(x) == 1`.\n",
        "    \"\"\"\n",
        "    ndim = K.ndim(x)\n",
        "    if ndim == 2:\n",
        "        return K.softmax(x)\n",
        "    elif ndim > 2:\n",
        "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
        "        s = K.sum(e, axis=axis, keepdims=True)\n",
        "        return e / s\n",
        "    else:\n",
        "        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n",
        "        \n",
        "\n",
        "def plot_attention_map(model, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 6, Tx = 30, Ty = 10):\n",
        "    \"\"\"\n",
        "    Plot the attention map.\n",
        "  \n",
        "    \"\"\"\n",
        "    attention_map = np.zeros((10, 30))\n",
        "    Ty, Tx = attention_map.shape\n",
        "    \n",
        "    s0 = np.zeros((1, n_s))\n",
        "    c0 = np.zeros((1, n_s))\n",
        "    layer = model.layers[num]\n",
        "\n",
        "    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n",
        "    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n",
        "\n",
        "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(Ty)])\n",
        "    r = f([encoded, s0, c0])\n",
        "    \n",
        "    for t in range(Ty):\n",
        "        for t_prime in range(Tx):\n",
        "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
        "\n",
        "    # Normalize attention map\n",
        "#     row_max = attention_map.max(axis=1)\n",
        "#     attention_map = attention_map / row_max[:, None]\n",
        "\n",
        "    prediction = model.predict([encoded, s0, c0])\n",
        "    \n",
        "    predicted_text = []\n",
        "    for i in range(len(prediction)):\n",
        "        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
        "        \n",
        "    predicted_text = list(predicted_text)\n",
        "    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
        "    text_ = list(text)\n",
        "    \n",
        "    # get the lengths of the string\n",
        "    input_length = len(text)\n",
        "    output_length = Ty\n",
        "    \n",
        "    # Plot the attention_map\n",
        "    plt.clf()\n",
        "    f = plt.figure(figsize=(8, 8.5))\n",
        "    ax = f.add_subplot(1, 1, 1)\n",
        "\n",
        "    # add image\n",
        "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
        "\n",
        "    # add colorbar\n",
        "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
        "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
        "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
        "\n",
        "    # add labels\n",
        "    ax.set_yticks(range(output_length))\n",
        "    ax.set_yticklabels(predicted_text[:output_length])\n",
        "\n",
        "    ax.set_xticks(range(input_length))\n",
        "    ax.set_xticklabels(text_[:input_length], rotation=45)\n",
        "\n",
        "    ax.set_xlabel('Input Sequence')\n",
        "    ax.set_ylabel('Output Sequence')\n",
        "\n",
        "    # add grid and legend\n",
        "    ax.grid()\n",
        "\n",
        "    #f.show()\n",
        "    \n",
        "    return attention_map"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOhFdG8Lp5Yr",
        "outputId": "651bf0a0-a38e-46e8-df3f-ea34a4db1354"
      },
      "source": [
        "m = 10000\n",
        "dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)\n",
        "dataset[:10]\n",
        "human_vocab\n",
        "machine_vocab"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10000/10000 [00:00<00:00, 15060.91it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZGWprQJs3xd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a821a17-dc5e-4d73-d648-a2d956fd35fa"
      },
      "source": [
        "Tx = 30\n",
        "Ty = 10\n",
        "len(human_vocab)\n",
        "\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
        "\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)\n",
        "\n",
        "index = 0\n",
        "print(\"Source date:\", dataset[index][0])\n",
        "print(\"Target date:\", dataset[index][1])\n",
        "print()\n",
        "print(\"Source after preprocessing (indices):\", X[index])\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\n",
        "print()\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (10000, 30)\n",
            "Y.shape: (10000, 10)\n",
            "Xoh.shape: (10000, 30, 37)\n",
            "Yoh.shape: (10000, 10, 11)\n",
            "Source date: 13 february 2006\n",
            "Target date: 2006-02-13\n",
            "\n",
            "Source after preprocessing (indices): [ 4  6  0 18 17 14 28 31 13 28 34  0  5  3  3  9 36 36 36 36 36 36 36 36\n",
            " 36 36 36 36 36 36]\n",
            "Target after preprocessing (indices): [3 1 1 7 0 1 3 0 2 4]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TpeCgb03GBw"
      },
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor1 = Dense(10, activation = \"tanh\")\n",
        "densor2 = Dense(1, activation = \"relu\")\n",
        "activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n",
        "dotor = Dot(axes = 1)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC0Qmc3h3UME"
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    s_prev = repeator(s_prev)\n",
        "    concat = concatenator([a, s_prev])\n",
        "    e = densor1(concat)\n",
        "    energies = densor2(e)\n",
        "    alphas = activator(energies)\n",
        "    context = dotor([alphas, a])\n",
        "    return context"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8tp_Goq3diL",
        "outputId": "ec5c20c7-b8bf-4661-f769-4cb4e91e5982"
      },
      "source": [
        "n_a = 32\n",
        "n_s = 64\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\n",
        "output_layer = Dense(len(machine_vocab), activation=softmax)\n",
        "\n",
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    X = Input(shape=(Tx, human_vocab_size))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    outputs = []\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(X)\n",
        "    for t in range(Ty):\n",
        "        context = one_step_attention(a, s)\n",
        "        s, _, c = post_activation_LSTM_cell(inputs = context, initial_state= [s, c])\n",
        "        out = output_layer(s)\n",
        "        outputs.append(out)\n",
        "    model = Model(inputs = [X, s0, c0], outputs = outputs)\n",
        "    return model\n",
        "\n",
        "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 30, 37)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 30, 64)       17920       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_1 (RepeatVector)  (None, 30, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_2[0][0]                     \n",
            "                                                                 lstm_2[1][0]                     \n",
            "                                                                 lstm_2[2][0]                     \n",
            "                                                                 lstm_2[3][0]                     \n",
            "                                                                 lstm_2[4][0]                     \n",
            "                                                                 lstm_2[5][0]                     \n",
            "                                                                 lstm_2[6][0]                     \n",
            "                                                                 lstm_2[7][0]                     \n",
            "                                                                 lstm_2[8][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 30, 128)      0           bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[0][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[1][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[2][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[3][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[4][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[5][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[6][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[7][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[8][0]            \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 repeat_vector_1[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 30, 10)       1290        concatenate_1[0][0]              \n",
            "                                                                 concatenate_1[1][0]              \n",
            "                                                                 concatenate_1[2][0]              \n",
            "                                                                 concatenate_1[3][0]              \n",
            "                                                                 concatenate_1[4][0]              \n",
            "                                                                 concatenate_1[5][0]              \n",
            "                                                                 concatenate_1[6][0]              \n",
            "                                                                 concatenate_1[7][0]              \n",
            "                                                                 concatenate_1[8][0]              \n",
            "                                                                 concatenate_1[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 30, 1)        11          dense_3[0][0]                    \n",
            "                                                                 dense_3[1][0]                    \n",
            "                                                                 dense_3[2][0]                    \n",
            "                                                                 dense_3[3][0]                    \n",
            "                                                                 dense_3[4][0]                    \n",
            "                                                                 dense_3[5][0]                    \n",
            "                                                                 dense_3[6][0]                    \n",
            "                                                                 dense_3[7][0]                    \n",
            "                                                                 dense_3[8][0]                    \n",
            "                                                                 dense_3[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 30, 1)        0           dense_4[0][0]                    \n",
            "                                                                 dense_4[1][0]                    \n",
            "                                                                 dense_4[2][0]                    \n",
            "                                                                 dense_4[3][0]                    \n",
            "                                                                 dense_4[4][0]                    \n",
            "                                                                 dense_4[5][0]                    \n",
            "                                                                 dense_4[6][0]                    \n",
            "                                                                 dense_4[7][0]                    \n",
            "                                                                 dense_4[8][0]                    \n",
            "                                                                 dense_4[9][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot_1 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 64), (None,  33024       dot_1[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_1[1][0]                      \n",
            "                                                                 lstm_2[0][0]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "                                                                 dot_1[2][0]                      \n",
            "                                                                 lstm_2[1][0]                     \n",
            "                                                                 lstm_2[1][2]                     \n",
            "                                                                 dot_1[3][0]                      \n",
            "                                                                 lstm_2[2][0]                     \n",
            "                                                                 lstm_2[2][2]                     \n",
            "                                                                 dot_1[4][0]                      \n",
            "                                                                 lstm_2[3][0]                     \n",
            "                                                                 lstm_2[3][2]                     \n",
            "                                                                 dot_1[5][0]                      \n",
            "                                                                 lstm_2[4][0]                     \n",
            "                                                                 lstm_2[4][2]                     \n",
            "                                                                 dot_1[6][0]                      \n",
            "                                                                 lstm_2[5][0]                     \n",
            "                                                                 lstm_2[5][2]                     \n",
            "                                                                 dot_1[7][0]                      \n",
            "                                                                 lstm_2[6][0]                     \n",
            "                                                                 lstm_2[6][2]                     \n",
            "                                                                 dot_1[8][0]                      \n",
            "                                                                 lstm_2[7][0]                     \n",
            "                                                                 lstm_2[7][2]                     \n",
            "                                                                 dot_1[9][0]                      \n",
            "                                                                 lstm_2[8][0]                     \n",
            "                                                                 lstm_2[8][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 11)           715         lstm_2[0][0]                     \n",
            "                                                                 lstm_2[1][0]                     \n",
            "                                                                 lstm_2[2][0]                     \n",
            "                                                                 lstm_2[3][0]                     \n",
            "                                                                 lstm_2[4][0]                     \n",
            "                                                                 lstm_2[5][0]                     \n",
            "                                                                 lstm_2[6][0]                     \n",
            "                                                                 lstm_2[7][0]                     \n",
            "                                                                 lstm_2[8][0]                     \n",
            "                                                                 lstm_2[9][0]                     \n",
            "==================================================================================================\n",
            "Total params: 52,960\n",
            "Trainable params: 52,960\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M73at4UGgEx5"
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "opt = optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "### END CODE HERE ###"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA51UaiMIsCa"
      },
      "source": [
        "s0 = np.zeros((m, n_s))\n",
        "c0 = np.zeros((m, n_s))\n",
        "outputs = list(Yoh.swapaxes(0,1))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UABHDhby3k16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a7f4da0-4f2f-4d1c-d418-8a25a71d716f"
      },
      "source": [
        "model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100/100 [==============================] - 30s 86ms/step - loss: 21.9823 - dense_5_loss: 2.2471 - dense_5_1_loss: 2.0723 - dense_5_2_loss: 2.2981 - dense_5_3_loss: 2.6080 - dense_5_4_loss: 1.7495 - dense_5_5_loss: 1.9042 - dense_5_6_loss: 2.6204 - dense_5_7_loss: 1.6913 - dense_5_8_loss: 2.0955 - dense_5_9_loss: 2.6958 - dense_5_accuracy: 0.1290 - dense_5_1_accuracy: 0.3707 - dense_5_2_accuracy: 0.1903 - dense_5_3_accuracy: 0.1065 - dense_5_4_accuracy: 0.5219 - dense_5_5_accuracy: 0.2301 - dense_5_6_accuracy: 0.0395 - dense_5_7_accuracy: 0.6064 - dense_5_8_accuracy: 0.1213 - dense_5_9_accuracy: 0.0528\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2450793d50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZFzL8MTI3SX"
      },
      "source": [
        "model.load_weights('model.h5')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "YD3yg1viJbSw",
        "outputId": "f80978a4-2890-485b-e7d1-264f0699805a"
      },
      "source": [
        "EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n",
        "for example in EXAMPLES:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human_vocab)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n",
        "    print(len(human_vocab))\n",
        "    print(source.shape)\n",
        "    source = source.swapaxes(0,1)\n",
        "    print(source.shape)\n",
        "\n",
        "    prediction = model.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine_vocab[int(i)] for i in prediction]\n",
        "    \n",
        "    print(\"source:\", example)\n",
        "    print(\"output:\", ''.join(output))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37\n",
            "(30, 37)\n",
            "(37, 30)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-31280da512f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minv_machine_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;31m# If batch_size is not passed but steps is, calculate from the input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1527\u001b[0m           label, \", \".join(str(i.shape[0]) for i in nest.flatten(single_data)))\n\u001b[1;32m   1528\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 37, 10000, 10000\nMake sure all arrays contain the same number of samples."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "kS7KGSoQ3xZZ",
        "outputId": "63f25d6f-1297-40ba-bca1-8e9309e50b0c"
      },
      "source": [
        "model.summary()  \n",
        "\n",
        "attention_map = plot_attention_map(model, human_vocab, inv_machine_vocab, \"Tuesday 09 Oct 1993\", num = 7, n_s = 64)\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(37, 30) 3 May 1979\n",
            "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-6ea6c8a96db3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhuman_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minv_machine_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1606\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m         \"recommend using that to load a Dataset instead.\")\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGenericArrayLikeDataAdapter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m     \u001b[0m_check_data_cardinality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'NoneType'"
          ]
        }
      ]
    }
  ]
}